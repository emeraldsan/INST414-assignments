import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load your dataset into a DataFrame (replace 'your_data.csv' with your actual data file)
df = pd.read_csv('your_data.csv')

# Select relevant features for clustering
features = df[['purchase_frequency', 'average_purchase_amount', 'product_categories']]

# Handle categorical variables (if needed)
# features = pd.get_dummies(features, columns=['product_categories'])

# Standardize numerical features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Reduce dimensionality with PCA (if needed)
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(scaled_features)

# Determine optimal K using the Elbow Method
wcss = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method graph
plt.plot(range(1, 11), wcss, marker='o')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.show()

# Based on the Elbow Method, choose the optimal K value (e.g., K=4)
optimal_k = 4

# Perform K-Means clustering
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
df['cluster'] = kmeans.fit_predict(scaled_features)

# Characterize each cluster
cluster_characteristics = df.groupby('cluster').mean()

# Print the cluster characteristics
print("Cluster Characteristics:")
print(cluster_characteristics)

# Visualize the clusters (2D PCA plot)
plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=df['cluster'], cmap='viridis')
plt.title('Customer Clusters - 2D PCA Plot')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
